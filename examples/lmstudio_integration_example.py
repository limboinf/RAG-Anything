"""
LM Studio ä¸ RAG-Anything é›†æˆç¤ºä¾‹

æ­¤ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•å°† LM Studio ä¸ RAG-Anything é›†æˆä»¥è¿›è¡Œæœ¬åœ°æ–‡æœ¬æ–‡æ¡£å¤„ç†å’ŒæŸ¥è¯¢ã€‚

è¦æ±‚ï¼š
- æœ¬åœ°è¿è¡Œçš„ LM Studio å¹¶å¯ç”¨æœåŠ¡å™¨
- OpenAI Python åŒ…ï¼špip install openai
- å·²å®‰è£… RAG-Anythingï¼špip install raganything

ç¯å¢ƒè®¾ç½®ï¼š
åˆ›å»ºåŒ…å«ä»¥ä¸‹å†…å®¹çš„ .env æ–‡ä»¶ï¼š
LLM_BINDING=lmstudio
LLM_MODEL=openai/gpt-oss-20b
LLM_BINDING_HOST=http://localhost:1234/v1
LLM_BINDING_API_KEY=lm-studio
EMBEDDING_BINDING=lmstudio
EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5
EMBEDDING_BINDING_HOST=http://localhost:1234/v1
EMBEDDING_BINDING_API_KEY=lm-studio
"""

import os
import uuid
import asyncio
from typing import List, Dict, Optional
from dotenv import load_dotenv
from openai import AsyncOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# RAG-Anything å¯¼å…¥
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.utils import EmbeddingFunc
from lightrag.llm.openai import openai_complete_if_cache

LM_BASE_URL = os.getenv("LLM_BINDING_HOST", "http://localhost:1234/v1")
LM_API_KEY = os.getenv("LLM_BINDING_API_KEY", "lm-studio")
LM_MODEL_NAME = os.getenv("LLM_MODEL", "openai/gpt-oss-20b")
LM_EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-nomic-embed-text-v1.5")


async def lmstudio_llm_model_func(
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[Dict] = None,
    **kwargs,
) -> str:
    """LightRAG çš„é¡¶å±‚ LLM å‡½æ•°ï¼ˆå¯ pickle åºåˆ—åŒ–ï¼‰ã€‚"""
    return await openai_complete_if_cache(
        model=LM_MODEL_NAME,
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages or [],
        base_url=LM_BASE_URL,
        api_key=LM_API_KEY,
        **kwargs,
    )


async def lmstudio_embedding_async(texts: List[str]) -> List[List[float]]:
    """LightRAG çš„é¡¶å±‚åµŒå…¥å‡½æ•°ï¼ˆå¯ pickle åºåˆ—åŒ–ï¼‰ã€‚"""
    from lightrag.llm.openai import openai_embed

    embeddings = await openai_embed(
        texts=texts,
        model=LM_EMBED_MODEL,
        base_url=LM_BASE_URL,
        api_key=LM_API_KEY,
    )
    return embeddings.tolist()


class LMStudioRAGIntegration:
    """LM Studio ä¸ RAG-Anything çš„é›†æˆç±»ã€‚"""

    def __init__(self):
        # ä½¿ç”¨æ ‡å‡† LLM_BINDING å˜é‡çš„ LM Studio é…ç½®
        self.base_url = os.getenv("LLM_BINDING_HOST", "http://localhost:1234/v1")
        self.api_key = os.getenv("LLM_BINDING_API_KEY", "lm-studio")
        self.model_name = os.getenv("LLM_MODEL", "openai/gpt-oss-20b")
        self.embedding_model = os.getenv(
            "EMBEDDING_MODEL", "text-embedding-nomic-embed-text-v1.5"
        )

        # RAG-Anything é…ç½®
        # æ¯æ¬¡è¿è¡Œä½¿ç”¨æ–°çš„å·¥ä½œç›®å½•ä»¥é¿å…æ—§ç‰ˆ doc_status æ¨¡å¼å†²çª
        self.config = RAGAnythingConfig(
            working_dir=f"./rag_storage_lmstudio/{uuid.uuid4()}",
            parser="mineru",
            parse_method="auto",
            enable_image_processing=False,
            enable_table_processing=True,
            enable_equation_processing=True,
        )
        print(f"ğŸ“ Using working_dir: {self.config.working_dir}")

        self.rag = None

    async def test_connection(self) -> bool:
        """æµ‹è¯• LM Studio è¿æ¥ã€‚"""
        try:
            print(f"ğŸ”Œ Testing LM Studio connection at: {self.base_url}")
            client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)
            models = await client.models.list()
            print(f"âœ… Connected successfully! Found {len(models.data)} models")

            # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
            print("ğŸ“Š Available models:")
            for i, model in enumerate(models.data[:5]):
                marker = "ğŸ¯" if model.id == self.model_name else "  "
                print(f"{marker} {i+1}. {model.id}")

            if len(models.data) > 5:
                print(f"  ... and {len(models.data) - 5} more models")

            return True
        except Exception as e:
            print(f"âŒ Connection failed: {str(e)}")
            print("\nğŸ’¡ Troubleshooting tips:")
            print("1. Ensure LM Studio is running")
            print("2. Start the local server in LM Studio")
            print("3. Load a model or enable just-in-time loading")
            print(f"4. Verify server address: {self.base_url}")
            return False
        finally:
            try:
                await client.close()
            except Exception:
                pass

    async def test_chat_completion(self) -> bool:
        """æµ‹è¯•åŸºæœ¬èŠå¤©åŠŸèƒ½ã€‚"""
        try:
            print(f"ğŸ’¬ Testing chat with model: {self.model_name}")
            client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)
            response = await client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant."},
                    {
                        "role": "user",
                        "content": "Hello! Please confirm you're working and tell me your capabilities.",
                    },
                ],
                max_tokens=100,
                temperature=0.7,
            )

            result = response.choices[0].message.content.strip()
            print("âœ… Chat test successful!")
            print(f"Response: {result}")
            return True
        except Exception as e:
            print(f"âŒ Chat test failed: {str(e)}")
            return False
        finally:
            try:
                await client.close()
            except Exception:
                pass

    # å·²ç§»é™¤å·²å¼ƒç”¨çš„å·¥å‚è¾…åŠ©å‡½æ•°ä»¥å‡å°‘å†—ä½™

    def embedding_func_factory(self):
        """åˆ›å»ºå®Œå…¨å¯åºåˆ—åŒ–çš„åµŒå…¥å‡½æ•°ã€‚"""
        return EmbeddingFunc(
            embedding_dim=768,  # nomic-embed-text-v1.5 é»˜è®¤ç»´åº¦
            max_token_size=8192,  # nomic-embed-text-v1.5 ä¸Šä¸‹æ–‡é•¿åº¦
            func=lmstudio_embedding_async,
        )

    async def initialize_rag(self):
        """ä½¿ç”¨ LM Studio å‡½æ•°åˆå§‹åŒ– RAG-Anythingã€‚"""
        print("Initializing RAG-Anything with LM Studio...")

        try:
            self.rag = RAGAnything(
                config=self.config,
                llm_model_func=lmstudio_llm_model_func,
                embedding_func=self.embedding_func_factory(),
            )

            # å…¼å®¹æ€§ï¼šé¿å…å°†æœªçŸ¥å­—æ®µ 'multimodal_processed' å†™å…¥ LightRAG doc_status
            # è¾ƒæ—§çš„ LightRAG ç‰ˆæœ¬å¯èƒ½ä¸æ¥å— DocProcessingStatus ä¸­çš„æ­¤é¢å¤–å­—æ®µ
            async def _noop_mark_multimodal(doc_id: str):
                return None

            self.rag._mark_multimodal_processing_complete = _noop_mark_multimodal

            print("âœ… RAG-Anything initialized successfully!")
            return True
        except Exception as e:
            print(f"âŒ RAG initialization failed: {str(e)}")
            return False

    async def process_document_example(self, file_path: str):
        """ç¤ºä¾‹ï¼šä½¿ç”¨ LM Studio åç«¯å¤„ç†æ–‡æ¡£ã€‚"""
        if not self.rag:
            print("âŒ RAG not initialized. Call initialize_rag() first.")
            return

        try:
            print(f"ğŸ“„ Processing document: {file_path}")
            await self.rag.process_document_complete(
                file_path=file_path,
                output_dir="./output_lmstudio",
                parse_method="auto",
                display_stats=True,
            )
            print("âœ… Document processing completed!")
        except Exception as e:
            print(f"âŒ Document processing failed: {str(e)}")

    async def query_examples(self):
        """ä½¿ç”¨ä¸åŒæ¨¡å¼çš„æŸ¥è¯¢ç¤ºä¾‹ã€‚"""
        if not self.rag:
            print("âŒ RAG not initialized. Call initialize_rag() first.")
            return

        # æŸ¥è¯¢ç¤ºä¾‹
        queries = [
            ("What are the main topics in the processed documents?", "hybrid"),
            ("Summarize any tables or data found in the documents", "local"),
            ("What images or figures are mentioned?", "global"),
        ]

        print("\nğŸ” Running example queries...")
        for query, mode in queries:
            try:
                print(f"\nQuery ({mode}): {query}")
                result = await self.rag.aquery(query, mode=mode)
                print(f"Answer: {result[:200]}...")
            except Exception as e:
                print(f"âŒ Query failed: {str(e)}")

    async def simple_query_example(self):
        """ä½¿ç”¨ç¤ºä¾‹å†…å®¹çš„åŸºæœ¬æ–‡æœ¬æŸ¥è¯¢ç¤ºä¾‹ã€‚"""
        if not self.rag:
            print("âŒ RAG not initialized")
            return

        try:
            print("\nAdding sample content for testing...")

            # åˆ›å»º RAGAnything æœŸæœ›æ ¼å¼çš„å†…å®¹åˆ—è¡¨
            content_list = [
                {
                    "type": "text",
                    "text": """LM Studio Integration with RAG-Anything

This integration demonstrates how to connect LM Studio's local AI models with RAG-Anything's document processing capabilities. The system uses:

- LM Studio for local LLM inference
- nomic-embed-text-v1.5 for embeddings (768 dimensions)
- RAG-Anything for document processing and retrieval

Key benefits include:
- Privacy: All processing happens locally
- Performance: Direct API access to local models
- Flexibility: Support for various document formats
- Cost-effective: No external API usage""",
                    "page_idx": 0,
                }
            ]

            # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•æ’å…¥å†…å®¹åˆ—è¡¨
            await self.rag.insert_content_list(
                content_list=content_list,
                file_path="lmstudio_integration_demo.txt",
                # ä½¿ç”¨å”¯ä¸€çš„ doc_id ä»¥é¿å…å†²çªå’Œè·¨è¿è¡Œé‡ç”¨ doc_status
                doc_id=f"demo-content-{uuid.uuid4()}",
                display_stats=True,
            )
            print("âœ… Sample content added to knowledge base")

            print("\nTesting basic text query...")

            # ç®€å•æ–‡æœ¬æŸ¥è¯¢ç¤ºä¾‹
            result = await self.rag.aquery(
                "What are the key benefits of this LM Studio integration?",
                mode="hybrid",
            )
            print(f"âœ… Query result: {result[:300]}...")

        except Exception as e:
            print(f"âŒ Query failed: {str(e)}")


async def main():
    """ä¸»ç¤ºä¾‹å‡½æ•°ã€‚"""
    print("=" * 70)
    print("LM Studio + RAG-Anything Integration Example")
    print("=" * 70)

    # åˆå§‹åŒ–é›†æˆ
    integration = LMStudioRAGIntegration()

    # æµ‹è¯•è¿æ¥
    if not await integration.test_connection():
        return False

    print()
    if not await integration.test_chat_completion():
        return False

    # åˆå§‹åŒ– RAG
    print("\n" + "â”€" * 50)
    if not await integration.initialize_rag():
        return False

    # æ–‡æ¡£å¤„ç†ç¤ºä¾‹ï¼ˆå–æ¶ˆæ³¨é‡Šå¹¶æä¾›çœŸå®æ–‡ä»¶è·¯å¾„ï¼‰
    # await integration.process_document_example("path/to/your/document.pdf")

    # æŸ¥è¯¢ç¤ºä¾‹ï¼ˆå¤„ç†æ–‡æ¡£åå–æ¶ˆæ³¨é‡Šï¼‰
    # await integration.query_examples()

    # åŸºæœ¬æŸ¥è¯¢ç¤ºä¾‹
    await integration.simple_query_example()

    print("\n" + "=" * 70)
    print("Integration example completed successfully!")
    print("=" * 70)

    return True


if __name__ == "__main__":
    print("ğŸš€ Starting LM Studio integration example...")
    success = asyncio.run(main())

    exit(0 if success else 1)
